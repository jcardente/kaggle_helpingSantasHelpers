#+Title: Santa Helpers Kaggle Challenge
#+Author: John Cardente
#+Date: December 2014
#+startup: showall, indent


* LINKS
- http://www.kaggle.com/c/helping-santas-helpers
- https://github.com/noahvanhoucke/HelpingSantasHelpers

* NOTES
** <2014-12-02 Tue>

Run of example Python submission evaluation script

#+BEGIN_EXAMPLE
Success!
  Score = 1875730155.06
total time = 4302.30815291
#+END_EXAMPLE

** <2014-12-08 Mon>

Results running my Julia version of teh evaluation script

#+BEGIN_EXAMPLE
Success!
Score = 1.8757301550575209e9
total runtime = 909.4081900119781
#+END_EXAMPLE

And these are the results of evaluating my output.

#+BEGIN_EXAMPLE
Success!
Score = 1875730155.06
total runtime = 1523.191195011139
#+END_EXAMPLE

** <2014-12-09 Tue>

Testing Julia port before uploading to GitHub

Evaluation against sample solution
#+BEGIN_EXAMPLE
Success!
Score = 1875730155.06
total runtime = 1100.3616909980774
#+END_EXAMPLE


Output from my solution

#+BEGIN_EXAMPLE
julia SantasHelpers_NaiveSolution.jl toys_rev2.csv mysoln.csv
total runtime = 208.75394797325134
#+END_EXAMPLE

Evaluation against my solution

#+BEGIN_EXAMPLE
Success!
Score = 1875730155.06
total runtime = 985.7524769306183
#+END_EXAMPLE

** <2014-12-11 Thu>

Starting some simple exploratory data analysis,

#+BEGIN_EXAMPLE

> sum(as.numeric(data$Duration))
[1] 26003950765

> totalhrs/1000000 
[1] 26003.95

> summary(data$Duration)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1      21      69    2600     468   47470 


# R
foo <- function(rating, sanctioned, unsanctioned) {
  max(0.25, min(4.0, rating * 
                     (1.02 ^(sanctioned/60)) *
                     (0.09 ^(unsanctioned/60))))
}


# Julia
function scale_rating(rating, sanctioned, unsanctioned)
 max(0.25,
    min(4.0, rating * (1.02 ^ (sanctioned/60.0)) *
            (0.90 ^ (unsanctioned/60.0))))
end


#Python
def foo(rating, sanctioned, unsanctioned):
    return max(0.25,
               min(4.0, rating * (1.02 ** (sanctioned/60.0)) *
               (0.90 ** (unsanctioned/60.0))))

#+END_EXAMPLE


#+BEGIN_EXAMPLE

update_rating <- function(rating, sanctioned, unsanctioned) {
  max(0.25, min(4.0, rating * 
                     (1.02 ^(sanctioned/60)) *
                     (0.09 ^(unsanctioned/60))))
}

score <- function( last_minute, num_elves) {
  last_minute * log(1.0 + num_elves)
}

total_minutes <- 26003950765
min_prod   <- 0.25
start_prod <- 1.0
max_prod   <- 4.0
max_elves  <- 900

# Fastest execution time

df <- expand.grid(seq(min_prod, max_prod, .25),
            seq(200, max_elves, 100))
colnames(df) <- c("Rating", "Elves")
df$score <- score(total_minutes/(df$Rating * df$Elves), df$Elves)

#+END_EXAMPLE

** <2014-12-12 Fri>

Simple calculation. Rating will increase if the ratio sanctioned minutes to unsanctioned minutes is larger than,

\begin{align*}
1.02^{sm} * 0.9^{um} &> 1\\
1.02^{sm} &> \frac{1}{0.9^{um}}\\
sm \, \log(1.02) &> um \, \log\left( \frac{1}{0.9} \right)\\
\frac{sm}{um} &> \frac{1}{\log(1.02)} \log\left( \frac{1}{0.9} \right)\\
\frac{sm}{um} &> \approx 5.3\\
\end{align*}

Might be an important metric while scheduling stuff.

Important to note that unsanctioned time requires rest. So, the
effective productivity is 1/2 for any unsanctioned time. 

Actually, its worse since rest time doesn't start until the next day. 
Which means that the time from when the work stops to the start of the
next day is "dead" time. 

Need a routine that, for a given duration and rating, determines the
optimal start times relative to a single day. 

Some simple calculations on productivity improvements

\begin{align*}
r \, 1.02^n &= 4\\
1.02^n &= \frac{4}{r}\\
n \log(1.02) &= \log\left(\frac{4}{r}\right)\\
n &= \frac{\log\left(\frac{4}{r}\right)}{\log(1.02)}
\end{align*}

This gives a way to predict how many sanctioned hours are required to get
back to peak productivity.



** <2014-12-12 Fri>

Experimenting a bit with simple shelf packing

#+BEGIN_EXAMPLE
myToys = read_toys("../data/smalltoys.csv", 99);

function best_start_window(work_duration)
  #local work_duration = int(ceil(duration / rating))

 if (work_duration < (hrs._minutes_in_24h))    
    # NB - Pick window that ensures rating adjustment
    #      is one or greater
    max_unsanctioned = int(floor(0.165 * work_duration))
    min_sanctioned   = work_duration - max_unsanctioned
    best_start       = hrs._day_start
    best_end         = hrs._day_end -1 - min_sanctioned
  else
    # Pick window that maximizes the number of sanctioned
    # hours.
    work_duration     = work_duration % hrs._minutes_in_24h
    best_start       = hrs._day_start
    best_end         = max(hrs._day_start,
                           hrs._day_end -1 - min(hrs._hours_per_day * 60, work_duration))
  end

(best_start, best_end)
end


function scale_rating(rating, sanctioned, unsanctioned)
 max(0.25,
    min(4.0, rating * (1.02 ^ (sanctioned/60.0)) *
            (0.90 ^ (unsanctioned/60.0))))
end

function score_toy(current_toy, current_elf) 
  elf_start = current_elf.next_available_time % hrs._minutes_in_24h
  work_duration = int(ceil(current_toy.duration / current_elf.rating))

  best_start, best_end = best_start_window(work_duration)

  score = 0
  if (best_start <= elf_start <= best_end)
      start_minute = max(best_start, elf_start)
      sanctioned, unsanctioned = get_sanctioned_breakdown(start_minute,work_duration)

      scaled_rating   = scale_rating(current_elf.rating, sanctioned, unsanctioned) / 4
      scaled_resttime = min(1.0, unsanctioned / (hrs._hours_per_day * 60))

      # Simple score that compares the relative increase in 
      # rating to the amount of resttime required.
      score = scaled_rating - scaled_resttime
  end

  score
end

function find_best_toy(current_elf, myToys)
  scores = [score_toy(t, current_elf) for t in values(myToys)] ./ current_elf.rating
  first(findin(scores, maximum(scores)))
end


#+END_EXAMPLE
** <2014-12-19 Fri>

Getting an idea of the distribution of toys

#+BEGIN_EXAMPLE
using hrs

myToys = readcsv("data/toys_rev2.csv", header=true)[1];

size(myToys)
(10000000,3)

myToys[:,2] = map(convert_to_minute, myToys[:,2]);

julia> maximum(myToys[:,3])
47473.0

julia> median(myToys[:,3])
69.0

julia> quantile(myToys[:,3],0.9)
12435.0

#+END_EXAMPLE

Switching to R

#+BEGIN_EXAMPLE


as.POSIXct(arrival, "%Y %m %d %H %M", tz = 'UTC')

myToys = read.csv("data/toys_rev2.csv", header=TRUE, stringsAsFactors=FALSE)

refdate <- as.POSIXct("2014 1 1 0 0", "%Y %m %d %H %M", tz = 'UTC')
myToys$Arrival_minute <- as.POSIXct(myToys$Arrival_time, "%Y %m %d %H %M", tz = 'UTC')
myToys$Day <- as.integer(difftime(myToys$Arrival_minute,refdate,units=c("days")))

arrivals.by.day <- aggregate(myToys$Day,list(myToys$Day), length)
colnames(arrivals.by.day) <- c("Day","Arrivals")


durations.by.day <- aggregate(myToys$Duration,list(myToys$Day), sum)
colnames(durations.by.day) <- c("Day", "Durations")

both.by.day <- merge(arrivals.by.day, durations.by.day, by="Day")
both.by.day$avg.size <- both.by.day$Durations / both.by.day$Arrivals

ggplot(both.by.day, aes(x=Day, y=Arrivals)) + geom_point()
ggplot(both.by.day, aes(x=Day, y=Durations)) + geom_point()
ggplot(both.by.day, aes(x=Day, y=avg.size)) + geom_point()

# Days 294 to 309 but indexes are off by one
both.by.day[295:310,]

bubble <- both.by.day[295:310,]


> colSums(bubble[,-1])
    Arrivals    Durations     avg.size 
2.250000e+06 2.518979e+10 1.791275e+05 

#+END_EXAMPLE

Days 295 to 310 are when the big burst hits - a 16 day span. 
By then, need elves that are highly productive and minimize sanctioned time.


Let's make this the training set!

#+BEGIN_EXAMPLE
> sum(foo$x[(foo$Group.1 > 280) & (foo$Group.1 < 295)])
[1] 455885
#+END_EXAMPLE


#+BEGIN_EXAMPLE
ratings <- seq(0.25, 4, 0.25)
nelfs <- seq(500,900,50)

testcases <- expand.grid(ratings, nelfs)
colnames(testcases) <- c("Rating","NELFS")

Arrivals <- bubble.all.df$Arrivals
Avg_size <- bubble.all.df$Avg_size

testcases$Minutes <- (Arrivals/testcases$NELFS) * (Avg_size/testcases$Rating)
testcases$Score <- testcases$Minutes + log(1 + testcases$NELFS)

ggplot(testcases,aes(x=NELFS, y=Rating, z=Score)) + stat_contour()

adjust_rating <- function(rating, sanctioned, unsanctioned) {
  max(0.25,
      min(4.0, rating * (1.02 ^ (sanctioned/60)) *
               (0.9 ^ (unsanctioned/60))))
}

sanctioned <- 10*60 * (Avg_size / (24*60))
unsanctioned <- 14 * 60 * (Avg_size / (24*60))

rating.ratio <- (1.02 ^ (sanctioned/60)) * (0.9 ^ (unsanctioned/60))

arrivals.per.day <- Arrivals/16
arrivals.per.day/900 * (Avg_size/0.25)
[1] 6997163

(Avg_size/4) * 900 + (Avg_size/0.25)* ((Arrivals-900)/900)


 (Avg_size/4) * 900 + (Avg_size/0.25)* ((Arrivals-900)/900)


est.score <- function (rating, nelves) {
 (Avg_size/rating) * nelves + (Avg_size/0.25)* ((Arrivals-nelves)/nelves)
}


testcases$Score2 <- est.score(testcases$Rating, testcases$NELFS)

ggplot(testcases,aes(x=NELFS, y=Rating, z=Score2)) + stat_contour()
ggplot(testcases,aes(x=NELFS, y=Rating, fill=Score2)) + geom_tile()


#+END_EXAMPLE


** <2014-12-22 Mon>

Oh fooey. Tried a full run over the weekend and it was super slow.
Need a new plan. Going to try batching up jobs into larger jobs to
make it easier to schedule.

First going to refactor the code a bit to separate out the agent
solution from the boiler plate command line execution code. 


#+BEGIN_EXAMPLE
maximum([t.arrival_minute for t in values(myToys)])

idxs = sort([k for k in keys(myToys)]);
arrivals = [myToys[i].arrival_minute for i in idxs];
arr1 = arrivals[1:length(arrivals)-1];
arr2 = arrivals[2:length(arrivals)];
all(arr2 .>= arr1)
  true
#+END_EXAMPLE

#+BEGIN_EXAMPLE



function tlt (a,b) 
       (a[2] <= b[2]) & (a[3] > b[3])
       end

toyinfo = [(tid, myToys[tid].arrival_minute, myToys[tid].duration) 
           for tid in 1:length(myToys)];
sort!(toyinfo, lt=tlt)

heights      = fill(540, 100)
counts       = zeros(length(heights))
total_work   = zeros(length(heights))
assignments  = Array(Any, length(heights))
for i in 1:length(heights)
  assignments[i] = Int[]
end
foo = Any[]
for tup in toyinfo
   arrival  = tup[2]
   duration = tup[3]

   #if duration > 60
   #  continue
   #end 

   wait_time = max(0, arrival .- heights)
   work_time = wait_time + duration
   rem_time  = 1140 - (heights + work_time)

   unsanctioned_time = map( x -> (x < 0) ? abs(x) : 0, rem_time)

   # If it doesn't fit nicely into any bucket then 
   # don't assign it.
   if (all(unsanctioned_time .> 30))
      continue
   end

   left_time = map( x -> (x > 0) ? x : 0, 
                          rem_time)

   (mt, idx) = findmin(left_time .+ wait_time .+ unsanctioned_time)

   this_wait_time    = wait_time[idx]
   this_unsanctioned = unsanctioned_time[idx]

   push!(assignments[idx], tup[1])
   heights[idx]    += this_wait_time + duration
   #total_work[idx] += work_time
   counts[idx]     += 1

   if ( this_unsanctioned > 0)
      push!(foo, assignments[idx])
      assignments[idx] = Int[]
      heights[idx]     = 540
      counts[idx]      = 0
   end
   
end 






function grokcombo(combolist)

current_time = 540
total_wait   = 0
for tid in combolist
  wait_time = max(0, myToys[tid].arrival_minute - current_time)
  duration  = myToys[tid].duration

  current_time += wait_time + duration
  total_wait   += wait_time
   
end

unsanctioned = max(0, current_time - 1140)

current_time, total_wait, unsanctioned

end



function which_bucket(duration::Integer)
  const limit = 5
  duration < limit ? duration : limit+int32(log10(duration))
end


durations = Int[t.duration for t in myToys]
#+END_EXAMPLE

** <2014-12-29 Mon>

Doing some experiments with 1M toys

700 elves:
Runtime= 88.73 	Score= 514352 	Prod=3.98	 LastMin=78497

800 elves:
Runtime= 93.07 	Score= 524820 	Prod=3.97	 LastMin=78497

Full 10M
